{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Video segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
   "metadata": {},
   "source": [
    "This notebook shows how to use SAM 2 to label objectid in successive frames given bounding box in first frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6aa9d-487f-4207-b657-8cff0902343e",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "\n",
      "Support for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Loading the SAM 2 video predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5320fe-06d7-45b8-b888-ae00799d07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='orange', facecolor=(0, 0, 0, 0), lw=1))\n",
    "\n",
    "\n",
    "def find_bounding_box(mask):\n",
    "    # Get the row, col indices of all True (non-zero) pixels\n",
    "    coords = np.argwhere(mask[0])\n",
    "    \n",
    "    # If the mask is empty, coords will be empty.\n",
    "    # Otherwise, compute bounding box:\n",
    "    if coords.size > 0:\n",
    "        # coords[:, 0] are the y indices (rows), coords[:, 1] are the x indices (columns)\n",
    "        y_min, x_min = coords.min(axis=0)\n",
    "        y_max, x_max = coords.max(axis=0)\n",
    "    \n",
    "        box = np.array([int(x_min), int(y_min), int(x_max), int(y_max)], dtype=np.float32)\n",
    "        # print(\"Bounding Box:\", box)\n",
    "        # box = np.array([1494, 349, 1857, 548], dtype=np.float32)\n",
    "        # show_box(box, ax)\n",
    "        return box\n",
    "    else:\n",
    "        print(\"Mask is empty, no bounding box found.\")\n",
    "        return None\n",
    "\n",
    "def show_bounding_box(mask, ax):\n",
    "    box = find_bounding_box(mask)\n",
    "    if box is not None:\n",
    "        show_box(box, ax)\n",
    "\n",
    "def get_annotation_frame_box(frame_annotation, frame_width, frame_height):\n",
    "    x_min = int(frame_annotation['x1'] / frame_annotation['width'] * frame_width)\n",
    "    y_min = int(frame_annotation['y1'] / frame_annotation['height'] * frame_height)\n",
    "    x_max = int(frame_annotation['x2'] / frame_annotation['width'] * frame_width)\n",
    "    y_max = int(frame_annotation['y2'] / frame_annotation['height'] * frame_height)\n",
    "    return [x_min, y_min, x_max, y_max] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {},
   "source": [
    "#### Select an example video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97771c36-b860-4643-b687-a69e9b79cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will apply SAM2 to annotate all frames given first frame annotation\n",
    "def get_annotations_from_video_frames(video_dir, frame_annotation, display_frames=False, vis_frame_stride = 5):\n",
    "    # `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "    \n",
    "    # scan all the JPEG frame names in this directory\n",
    "    frame_names = [\n",
    "        p for p in os.listdir(video_dir)\n",
    "        if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "    ]\n",
    "    frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "    # take a look the first video frame\n",
    "    frame_idx = 0\n",
    "    \n",
    "    # Get the height and width of first frame\n",
    "    with Image.open(os.path.join(video_dir, frame_names[frame_idx])) as img:\n",
    "        width, height = img.size  # img.size returns a tuple (width, height)\n",
    "        print(f\"Frame {frame_idx} - Width: {width}px, Height: {height}px\")\n",
    "\n",
    "    # Transform frame_annotation to initial bounding box\n",
    "    init_box = get_annotation_frame_box(frame_annotation, width, height)\n",
    "    \n",
    "    # Initialise the state of the predictor\n",
    "    inference_state = predictor.init_state(video_path=video_dir)\n",
    "\n",
    "    # Now annotate first frame\n",
    "    ann_frame_idx = 0  # the frame index we interact with\n",
    "    ann_obj_id = 0  # give a unique id to each object we interact with (it can be any integers)\n",
    "    \n",
    "    # Let's add a box at (x_min, y_min, x_max, y_max) e.g. (300, 0, 500, 400) to get started\n",
    "    box = np.array(init_box, dtype=np.float32)\n",
    "    \n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        box=box,\n",
    "    )\n",
    "\n",
    "    # Register the first frame value\n",
    "    first_frame_num = int(frame_names[0].split('.')[0])\n",
    "\n",
    "    # show the results on the current (interacted) frame\n",
    "    if display_frames:\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        plt.title(f\"frame {ann_frame_idx}\")\n",
    "        plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "        show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])\n",
    "        show_bounding_box((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca())\n",
    "\n",
    "    # run propagation throughout the video and collect the results in a dict\n",
    "    annotations = {}\n",
    "\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "\n",
    "        out_mask = (out_mask_logits[0] > 0.0).cpu().numpy()\n",
    "        \n",
    "        frame_num = int(frame_names[out_frame_idx].split('.')[0])\n",
    "        frame_count = frame_num - first_frame_num\n",
    "        box = find_bounding_box(out_mask)\n",
    "\n",
    "        # Display mask over base image\n",
    "        if display_frames & (frame_count % vis_frame_stride == 0):\n",
    "            plt.figure(figsize=(12, 9))\n",
    "            plt.title(f\"frame {frame_num} ({frame_count})\")\n",
    "            plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "            show_mask(out_mask, plt.gca(), obj_id=out_obj_ids[0])\n",
    "            show_bounding_box(out_mask, plt.gca())\n",
    "\n",
    "        if box is not None:\n",
    "            int_box = box.astype(np.int32).tolist()\n",
    "            annotation = bounding_box_to_annotation(int_box, width, height, frame_annotation)\n",
    "            if display_frames & (frame_count % vis_frame_stride == 0):\n",
    "                print('Frame: ', frame_num)\n",
    "                print('Box: ', int_box)\n",
    "                print('Annotation: ', annotation)\n",
    "            annotations[frame_num] = annotation\n",
    "\n",
    "    # Write the annotations dictionary to a JSON file\n",
    "    output_json_path = os.path.join(video_dir, 'annotations.json')\n",
    "    try:\n",
    "        with open(output_json_path, 'w') as json_file:\n",
    "            json.dump(annotations, json_file, indent=4)\n",
    "        print(f\"Annotations successfully written to {output_json_path}\")\n",
    "\n",
    "        return annotations\n",
    "    except IOError as e:\n",
    "        raise RuntimeError(f\"Failed to write annotations to {output_json_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09128a1a-3676-4877-963d-95db8f29fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_image_file(frame_path, number):\n",
    "    # Format the number with leading zeros to make it 5 digits\n",
    "    filename = f\"{number:05d}.jpg\"\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(frame_path, filename)\n",
    "    \n",
    "    # Delete the file\n",
    "    os.remove(file_path)\n",
    "    print(f\"Deleted file: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499adf76-e23a-4b52-b880-98f21da83c96",
   "metadata": {},
   "source": [
    "We assume that the video is stored as a list of JPEG frames with filenames like `<frame_index>.jpg`.\n",
    "\n",
    "For your custom videos, you can extract their JPEG frames using ffmpeg (https://ffmpeg.org/) as follows:\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 1000 -vframes 100 <output_dir>/'%05d.jpg'\n",
    "```\n",
    "where `-q:v` generates high-quality JPEG frames and `-start_number 1000` asks ffmpeg to start the JPEG file from `01000.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88100c7-23d9-40ae-9b66-45bfd7ac6bfa",
   "metadata": {},
   "source": [
    "To select only between designated frames we use the following:\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -vf \"select='between(n\\,1000\\,1099)'\" -fps_mode passthrough -q:v 2 -start_number 1000 <output_dir>/'%05d.jpg'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10701e83-b56c-47c1-b768-1b52e9840869",
   "metadata": {},
   "source": [
    "This ffmpeg command works from shell:\n",
    "\n",
    "```\n",
    "ffmpeg -i /Users/kjr/Python/GeoVideoTagging/KJR-DATA-02/Goondoi/Missions/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V.MP4 -vf \"select='between(n\\,1000\\,1099)'\" -fps_mode passthrough -q:v 2 -start_number 994 /tmp/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V/'%05d.jpg'\n",
    "```\n",
    "\n",
    "For more precise start time:\n",
    "```\n",
    "ffmpeg -ss 33.67 -i example.mp4 -q:v 2 -start_number 1000 -vframes 100 frames/'%05d.jpg'\n",
    "\n",
    "ffmpeg -ss 33.4005376344086 -i /Users/kjr/Python/GeoVideoTagging/KJR-DATA-02/Goondoi/Missions/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V.MP4 -q:v 2 -start_number 994 -vframes 100 /tmp/SAM2/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V_994_1003/%05d.jpg\n",
    "\n",
    "-vf \"fps=29.76\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bf75f65-bbeb-4da8-8446-024a33d2ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box_to_annotation(box, width, height, frame_annotation):\n",
    "    [x1, y1, x2, y2] = box\n",
    "\n",
    "    # Will provide a new annotation based on initial annotation, but with new location of bounding box\n",
    "    new_annotation = {\n",
    "        \"x1\": int(x1 / width * frame_annotation[\"width\"]),\n",
    "        \"y1\": int(y1 / height * frame_annotation[\"height\"]),\n",
    "        \"x2\": int(x2 / width * frame_annotation[\"width\"]),\n",
    "        \"y2\": int(y2 / height * frame_annotation[\"height\"]),\n",
    "        \"width\": frame_annotation[\"width\"],\n",
    "        \"height\": frame_annotation[\"height\"],\n",
    "        \"type\": frame_annotation[\"type\"],\n",
    "        \"tags\": frame_annotation[\"tags\"]\n",
    "    }\n",
    "    \n",
    "    return new_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "902565fe-b5a7-461f-8453-a972f8831c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the frames from the video using ffmpeg\n",
    "def generate_video_frames_using_ffmeg(src_video_file, video_fps, frame_start, frame_end, dst_frame_path):\n",
    "    # Determine source video\n",
    "    # src_video_path = os.path.join(gva_src, data_src, video_folder)\n",
    "    # src_video_file = os.path.join(src_video_path, f\"{video_file}.{video_extn}\")\n",
    "\n",
    "    # Check if it's a file\n",
    "    if os.path.isfile(src_video_file):\n",
    "        print(f\"File '{src_video_file}' exists and is a file.\")\n",
    "    else:        \n",
    "        # Raise an exception if the file does not exist\n",
    "        raise FileNotFoundError(f\"File '{src_video_file}' does not exist or is not a file.\")\n",
    "\n",
    "    # Determine target folder for frame generation\n",
    "    # dst_frame_path = os.path.join(tmp_folder, video_folder, f\"{video_file}_{frame_start}_{frame_end}\")\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(dst_frame_path, exist_ok=True)\n",
    "\n",
    "    # Setup ffmpeg command parameters\n",
    "    start_time = f\"{(frame_start - 1) / video_fps}\"\n",
    "    num_frames = f\"{frame_end - frame_start + 1 + 1}\"      # We add second +1 to account for ffmpeg -vframe repeats first frame\n",
    "    frame_rate = '\"fps={}\"'.format(video_fps)\n",
    "    start_number = f\"{frame_start - 1}\"                    # We -1 to account for ffmpeg -vframe repeats first frame\n",
    "    # select_pattern = f\"\\\"select=\\'between(n,{frame_start},{frame_end})\\'\\\"\"\n",
    "    output_pattern = f\"{dst_frame_path}/%05d.jpg\"\n",
    "\n",
    "    # ffmpeg -vstart option has a side effect that first frame is duplicated, and frames are then one behind\n",
    "    # We patch this by labelling output one frame earlier, then will delete the first frame\n",
    "    \n",
    "    # cmd_arr = ['ffmpeg', '-i', src_video_file, '-vf', select_pattern, '-fps_mode', 'passthrough', '-q:v', '2', '-start_number', f\"{frame_start}\", output_pattern]\n",
    "    cmd_arr = ['ffmpeg', '-ss', start_time, '-i', src_video_file, '-q:v', '2', '-start_number', start_number, '-vf', frame_rate, '-vframes', num_frames, output_pattern]\n",
    "    ffmpeg_cmd = ' '.join(cmd_arr)\n",
    "    print('Execute ffmpeg: ', ffmpeg_cmd)\n",
    "\n",
    "    # Run ffmpeg\n",
    "    ffmpeg_exit_code = os.system(ffmpeg_cmd)\n",
    "    print('Exit Code ffmpeg:', ffmpeg_exit_code)\n",
    "    if ffmpeg_exit_code != 0:\n",
    "        raise RuntimeError(f\"FFmpeg command failed with exit code {ffmpeg_exit_code}\")\n",
    "\n",
    "    # Delete the first frame as duplicate from -vframe option\n",
    "    delete_image_file(dst_frame_path, int(start_number))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52cda2-b980-4a57-a7de-1b295db5363c",
   "metadata": {},
   "source": [
    "## Generate bounding box for first frame\n",
    "\n",
    "We need to convert the GVA annotation to the bounding box to set for first frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd99e73-dfe4-497f-b879-deb44eefcedc",
   "metadata": {},
   "source": [
    "## Pull all routines together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f945c275-4454-4e83-bd07-8ec5b22501e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gva_src = '/Users/kjr/Python/GeoVideoTagging/'\n",
    "data_src = 'KJR-DATA-02/Goondoi/Missions/'\n",
    "video_folder = 'Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/'\n",
    "video_file = 'DJI_20241130073900_0001_V'\n",
    "video_extn = 'MP4'\n",
    "video_fps = 29.76\n",
    "frame_start = 994\n",
    "frame_end = 998\n",
    "tmp_folder = '/tmp/SAM2/'\n",
    "frame_annotation = {\n",
    "        \"x1\": 106,\n",
    "        \"y1\": 281,\n",
    "        \"x2\": 142,\n",
    "        \"y2\": 294,\n",
    "        \"id\": 0,\n",
    "        \"width\": 737,\n",
    "        \"height\": 415,\n",
    "        \"type\": \"rectangle\",\n",
    "        \"tags\": [\n",
    "            \"vessel\"\n",
    "        ],\n",
    "        \"name\": 1,\n",
    "        \"interpolated\": False,\n",
    "        \"objectid\": \"pilot1\",\n",
    "        \"depth\": 260\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d6a0348-3117-4d20-8eb0-4b68d0e49c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '/Users/kjr/Python/GeoVideoTagging/KJR-DATA-02/Goondoi/Missions/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V.MP4' exists and is a file.\n",
      "Execute ffmpeg:  ffmpeg -ss 33.36693548387097 -i /Users/kjr/Python/GeoVideoTagging/KJR-DATA-02/Goondoi/Missions/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V.MP4 -q:v 2 -start_number 993 -vf \"fps=29.76\" -vframes 6 /tmp/SAM2/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V_994_998/%05d.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 16.0.0 (clang-1600.0.26.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1_3 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x11ef04580] stream 0, timescale not set\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/Users/kjr/Python/GeoVideoTagging/KJR-DATA-02/Goondoi/Missions/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V.MP4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    creation_time   : 2024-11-29T23:39:01.000000Z\n",
      "    encoder         : DJI M3T\n",
      "  Duration: 00:05:50.15, start: 0.000000, bitrate: 85926 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 3840x2160, 84411 kb/s, 29.76 fps, 29.97 tbr, 30k tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-11-29T23:39:01.000000Z\n",
      "        handler_name    : VideoHandler\n",
      "        vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](und): Data: none (djmd / 0x646D6A64), 53 kb/s\n",
      "      Metadata:\n",
      "        creation_time   : 2024-11-29T23:39:01.000000Z\n",
      "        handler_name    : DJI meta\n",
      "  Stream #0:2[0x3](und): Data: none (dbgi / 0x69676264), 1442 kb/s\n",
      "      Metadata:\n",
      "        creation_time   : 2024-11-29T23:39:01.000000Z\n",
      "        handler_name    : DJI dbgi\n",
      "  Stream #0:3[0x0]: Video: mjpeg (Baseline), yuvj420p(pc, bt470bg/unknown/unknown), 960x540 [SAR 1:1 DAR 16:9], 90k tbr, 90k tbn (attached pic)\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
      "Press [q] to stop, [?] for help\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit Code ffmpeg: 0\n",
      "Deleted file: /tmp/SAM2/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V_994_998/00993.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Output #0, image2, to '/tmp/SAM2/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V_994_998/%05d.jpg':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf61.7.100\n",
      "  Stream #0:0(und): Video: mjpeg, yuv420p(pc, bt709, progressive), 3840x2160, q=2-31, 200 kb/s, 29.76 fps, 29.76 tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-11-29T23:39:01.000000Z\n",
      "        handler_name    : VideoHandler\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.19.100 mjpeg\n",
      "      Side data:\n",
      "        cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/image2 @ 0x600001220000] video:3503KiB audio:0KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: unknown\n",
      "frame=    6 fps=0.0 q=2.0 Lsize=N/A time=00:00:00.20 bitrate=N/A dup=1 drop=0 speed=0.776x    \n"
     ]
    }
   ],
   "source": [
    "src_video_file = os.path.join(gva_src, data_src, video_folder, f\"{video_file}.{video_extn}\")\n",
    "dst_frame_path = os.path.join(tmp_folder, video_folder, f\"{video_file}_{frame_start}_{frame_end}\")\n",
    "generate_video_frames_using_ffmeg(src_video_file, video_fps, frame_start, frame_end, dst_frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f3c881f-474a-4b67-9434-25684b24858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0 - Width: 3840px, Height: 2160px\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.42it/s]\n",
      "/Users/kjr/Python/SegmentAnything/sam2/myenv/lib/python3.11/site-packages/torch/nn/functional.py:4594: UserWarning: The operator 'aten::upsample_bicubic2d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch._C._nn.upsample_bicubic2d(\n",
      "/Users/kjr/Python/SegmentAnything/sam2/sam2/sam2_video_predictor.py:786: UserWarning: cannot import name '_C' from 'sam2' (/Users/kjr/Python/SegmentAnything/sam2/sam2/__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n",
      "propagate in video: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations successfully written to /tmp/SAM2/Tour_20241128/DJI_202411300738_001_Goondoi-flights-Innisfail/DJI_20241130073900_0001_V_994_998/annotations.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "annotations = get_annotations_from_video_frames(dst_frame_path, frame_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e00340f-7119-4bed-a185-57bceda9a6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': 156,\n",
       " 'y1': 279,\n",
       " 'x2': 190,\n",
       " 'y2': 291,\n",
       " 'width': 737,\n",
       " 'height': 415,\n",
       " 'type': 'rectangle',\n",
       " 'tags': ['vessel']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2c403-3428-4eeb-9533-a8d1d8518395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
